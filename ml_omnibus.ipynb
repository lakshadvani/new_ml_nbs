{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0-WS9tn7YEz8"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "K-Nearest Neighbors (KNN)\n",
        "\n",
        "K-Means Clustering\n",
        "\n",
        "Linear Regression\n",
        "\n",
        "Logistic Regression\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X  # list of lists\n",
        "        self.y_train = y  # list\n",
        "\n",
        "    def euclidean_distance(self, a, b):\n",
        "        total = 0\n",
        "        for i in range(len(a)):\n",
        "            total += (a[i] - b[i]) ** 2\n",
        "        return total ** 0.5\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            distances = []\n",
        "            for i in range(len(self.X_train)):\n",
        "                dist = self.euclidean_distance(x, self.X_train[i])\n",
        "                distances.append((dist, self.y_train[i]))\n",
        "            distances.sort()\n",
        "            k_nearest = [label for _, label in distances[:self.k]]\n",
        "\n",
        "            count = {}\n",
        "            for label in k_nearest:\n",
        "                if label not in count:\n",
        "                    count[label] = 0\n",
        "                count[label] += 1\n",
        "\n",
        "            best_label = max(count, key=count.get)\n",
        "            predictions.append(best_label)\n",
        "        return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = [[2, 2]]\n",
        "l2 = [0]\n",
        "knn = KNN()\n",
        "print(knn.fit(l1,l2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TRRpyjeZfS5",
        "outputId": "b6cdbdd6-8b6b-4f04-b89e-a08ec42f1419"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class KMeans:\n",
        "    def __init__(self, k=2, max_iters=100):\n",
        "        self.k = k\n",
        "        self.max_iters = max_iters\n",
        "\n",
        "    def fit(self, X):\n",
        "        self.centroids = random.sample(X, self.k)\n",
        "\n",
        "        for _ in range(self.max_iters):\n",
        "            clusters = [[] for _ in range(self.k)]\n",
        "            for point in X:\n",
        "                distances = []\n",
        "                for centroid in self.centroids:\n",
        "                    dist = sum((point[i] - centroid[i]) ** 2 for i in range(len(point))) ** 0.5\n",
        "                    distances.append(dist)\n",
        "                cluster_index = distances.index(min(distances))\n",
        "                clusters[cluster_index].append(point)\n",
        "\n",
        "            new_centroids = []\n",
        "            for cluster in clusters:\n",
        "                if not cluster:\n",
        "                    new_centroids.append(random.choice(X))\n",
        "                    continue\n",
        "                mean = []\n",
        "                for i in range(len(cluster[0])):\n",
        "                    col_sum = sum(point[i] for point in cluster)\n",
        "                    mean.append(col_sum / len(cluster))\n",
        "                new_centroids.append(mean)\n",
        "\n",
        "            if new_centroids == self.centroids:\n",
        "                break\n",
        "            self.centroids = new_centroids\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for point in X:\n",
        "            distances = []\n",
        "            for centroid in self.centroids:\n",
        "                dist = sum((point[i] - centroid[i]) ** 2 for i in range(len(point))) ** 0.5\n",
        "                distances.append(dist)\n",
        "            predictions.append(distances.index(min(distances)))\n",
        "        return predictions\n"
      ],
      "metadata": {
        "id": "xKFE6yVZZk0b"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression:\n",
        "    def __init__(self, lr=0.01, epochs=1000):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.n = len(X[0])\n",
        "        self.w = [0.0] * self.n\n",
        "        self.b = 0.0\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            dw = [0.0] * self.n\n",
        "            db = 0.0\n",
        "            for i in range(len(X)):\n",
        "                y_pred = sum(self.w[j] * X[i][j] for j in range(self.n)) + self.b\n",
        "                error = y_pred - y[i]\n",
        "                for j in range(self.n):\n",
        "                    dw[j] += error * X[i][j]\n",
        "                db += error\n",
        "\n",
        "            for j in range(self.n):\n",
        "                self.w[j] -= self.lr * dw[j] / len(X)\n",
        "            self.b -= self.lr * db / len(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            pred = sum(self.w[j] * x[j] for j in range(self.n)) + self.b\n",
        "            predictions.append(pred)\n",
        "        return predictions\n"
      ],
      "metadata": {
        "id": "oybk6FXMZudQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self, lr=0.01, epochs=1000):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + math.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.n = len(X[0])\n",
        "        self.w = [0.0] * self.n\n",
        "        self.b = 0.0\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            dw = [0.0] * self.n\n",
        "            db = 0.0\n",
        "            for i in range(len(X)):\n",
        "                z = sum(self.w[j] * X[i][j] for j in range(self.n)) + self.b\n",
        "                pred = self.sigmoid(z)\n",
        "                error = pred - y[i]\n",
        "                for j in range(self.n):\n",
        "                    dw[j] += error * X[i][j]\n",
        "                db += error\n",
        "\n",
        "            for j in range(self.n):\n",
        "                self.w[j] -= self.lr * dw[j] / len(X)\n",
        "            self.b -= self.lr * db / len(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            z = sum(self.w[j] * x[j] for j in range(self.n)) + self.b\n",
        "            pred = self.sigmoid(z)\n",
        "            predictions.append(1 if pred >= 0.5 else 0)\n",
        "        return predictions\n"
      ],
      "metadata": {
        "id": "rO6mX5P1ZvpN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################"
      ],
      "metadata": {
        "id": "vnfUbYT0ZwrW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree\n",
        "class DecisionTreeNode:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=3):\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def gini(self, groups, classes):\n",
        "        n_instances = sum(len(group) for group in groups)\n",
        "        gini = 0.0\n",
        "        for group in groups:\n",
        "            size = len(group)\n",
        "            if size == 0:\n",
        "                continue\n",
        "            score = 0.0\n",
        "            labels = [row[-1] for row in group]\n",
        "            for class_val in classes:\n",
        "                proportion = labels.count(class_val) / size\n",
        "                score += proportion ** 2\n",
        "            gini += (1 - score) * (size / n_instances)\n",
        "        return gini\n",
        "\n",
        "    def split(self, index, value, dataset):\n",
        "        left, right = [], []\n",
        "        for row in dataset:\n",
        "            if row[index] < value:\n",
        "                left.append(row)\n",
        "            else:\n",
        "                right.append(row)\n",
        "        return left, right\n",
        "\n",
        "    def best_split(self, dataset):\n",
        "        class_values = list(set(row[-1] for row in dataset))\n",
        "        best_index, best_value, best_score, best_groups = 999, 999, 999, None\n",
        "        for index in range(len(dataset[0]) - 1):\n",
        "            for row in dataset:\n",
        "                groups = self.split(index, row[index], dataset)\n",
        "                gini = self.gini(groups, class_values)\n",
        "                if gini < best_score:\n",
        "                    best_index, best_value, best_score, best_groups = index, row[index], gini, groups\n",
        "        return {'index': best_index, 'value': best_value, 'groups': best_groups}\n",
        "\n",
        "    def to_terminal(self, group):\n",
        "        outcomes = [row[-1] for row in group]\n",
        "        return max(set(outcomes), key=outcomes.count)\n",
        "\n",
        "    def build_tree(self, dataset, depth):\n",
        "        split = self.best_split(dataset)\n",
        "        left, right = split['groups']\n",
        "        node = DecisionTreeNode(feature=split['index'], threshold=split['value'])\n",
        "        if depth >= self.max_depth or not left or not right:\n",
        "            node.value = self.to_terminal(left + right)\n",
        "            return node\n",
        "        node.left = self.build_tree(left, depth + 1)\n",
        "        node.right = self.build_tree(right, depth + 1)\n",
        "        return node\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        dataset = [X[i] + [y[i]] for i in range(len(X))]\n",
        "        self.root = self.build_tree(dataset, 1)\n",
        "\n",
        "    def _predict(self, node, row):\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "        if row[node.feature] < node.threshold:\n",
        "            return self._predict(node.left, row)\n",
        "        else:\n",
        "            return self._predict(node.right, row)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self._predict(self.root, row) for row in X]\n",
        "\n",
        "\n",
        "# Naive Bayes for Gaussian distributions\n",
        "import math\n",
        "\n",
        "class NaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        self.classes = list(set(y))\n",
        "        self.summaries = {}\n",
        "        for c in self.classes:\n",
        "            features = [X[i] for i in range(len(X)) if y[i] == c]\n",
        "            summary = []\n",
        "            for i in range(len(X[0])):\n",
        "                col = [x[i] for x in features]\n",
        "                mean = sum(col) / len(col)\n",
        "                variance = sum((x - mean) ** 2 for x in col) / len(col)\n",
        "                summary.append((mean, variance))\n",
        "            self.summaries[c] = summary\n",
        "\n",
        "    def gaussian(self, x, mean, var):\n",
        "        if var == 0:\n",
        "            return 1.0 if x == mean else 0.0\n",
        "        exponent = math.exp(-(x - mean) ** 2 / (2 * var))\n",
        "        return (1 / math.sqrt(2 * math.pi * var)) * exponent\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for row in X:\n",
        "            probs = {}\n",
        "            for c in self.classes:\n",
        "                prob = 1\n",
        "                for i in range(len(row)):\n",
        "                    mean, var = self.summaries[c][i]\n",
        "                    prob *= self.gaussian(row[i], mean, var)\n",
        "                probs[c] = prob\n",
        "            predictions.append(max(probs, key=probs.get))\n",
        "        return predictions\n",
        "\n",
        "\n",
        "# Run test cases for Decision Tree and Naive Bayes\n",
        "\n",
        "# Decision Tree test (binary feature)\n",
        "X_tree = [[2.7], [1.0], [3.0], [1.5]]\n",
        "y_tree = [0, 0, 1, 1]\n",
        "model_tree = DecisionTree(max_depth=2)\n",
        "model_tree.fit(X_tree, y_tree)\n",
        "tree_result = model_tree.predict([[2.0], [3.1]])\n",
        "\n",
        "# Naive Bayes test (Gaussian)\n",
        "X_nb = [[1.0, 20.0], [2.0, 21.0], [3.0, 22.0], [4.0, 23.0]]\n",
        "y_nb = [0, 0, 1, 1]\n",
        "model_nb = NaiveBayes()\n",
        "model_nb.fit(X_nb, y_nb)\n",
        "nb_result = model_nb.predict([[1.5, 20.5], [3.5, 22.5]])\n",
        "\n",
        "(tree_result, nb_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CkqHqpGZ0oz",
        "outputId": "8295bdec-b6d3-47fd-ec6a-9acb6c03be12"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0, 1], [0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM with basic hard-margin using gradient descent for 2D data\n",
        "class SimpleSVM:\n",
        "    def __init__(self, lr=0.01, epochs=1000, lambda_param=0.01):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.lambda_param = lambda_param\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_features = len(X[0])\n",
        "        self.w = [0.0] * n_features\n",
        "        self.b = 0.0\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            for i in range(len(X)):\n",
        "                x_i = X[i]\n",
        "                y_i = 1 if y[i] == 1 else -1\n",
        "                condition = y_i * (sum(self.w[j] * x_i[j] for j in range(n_features)) + self.b) >= 1\n",
        "                if condition:\n",
        "                    for j in range(n_features):\n",
        "                        self.w[j] -= self.lr * (2 * self.lambda_param * self.w[j])\n",
        "                else:\n",
        "                    for j in range(n_features):\n",
        "                        self.w[j] -= self.lr * (2 * self.lambda_param * self.w[j] - y_i * x_i[j])\n",
        "                    self.b += self.lr * y_i\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [1 if sum(self.w[j] * x[j] for j in range(len(x))) + self.b >= 0 else 0 for x in X]\n",
        "\n",
        "\n",
        "# PCA using eigen decomposition (manual matrix operations for small data)\n",
        "def transpose(matrix):\n",
        "    return list(map(list, zip(*matrix)))\n",
        "\n",
        "def mean_vector(X):\n",
        "    return [sum(col) / len(col) for col in zip(*X)]\n",
        "\n",
        "def subtract_mean(X, mean_vec):\n",
        "    return [[x[i] - mean_vec[i] for i in range(len(x))] for x in X]\n",
        "\n",
        "def cov_matrix(X):\n",
        "    n = len(X)\n",
        "    X_T = transpose(X)\n",
        "    return [[sum(X_T[i][k] * X_T[j][k] for k in range(n)) / (n - 1)\n",
        "             for j in range(len(X_T))] for i in range(len(X_T))]\n",
        "\n",
        "def eigen_decomp_2x2(matrix):\n",
        "    # Only for 2x2 matrices\n",
        "    a, b = matrix[0][0], matrix[0][1]\n",
        "    c, d = matrix[1][0], matrix[1][1]\n",
        "    trace = a + d\n",
        "    det = a * d - b * c\n",
        "    lambda1 = trace / 2 + ((trace**2 - 4 * det) ** 0.5) / 2\n",
        "    lambda2 = trace / 2 - ((trace**2 - 4 * det) ** 0.5) / 2\n",
        "    # eigenvectors (not normalized)\n",
        "    v1 = [b, lambda1 - a] if b != 0 else [1, 0]\n",
        "    v2 = [b, lambda2 - a] if b != 0 else [0, 1]\n",
        "    return [(lambda1, v1), (lambda2, v2)]\n",
        "\n",
        "class SimplePCA:\n",
        "    def __init__(self, n_components=1):\n",
        "        self.n_components = n_components\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        self.mean_vec = mean_vector(X)\n",
        "        centered = subtract_mean(X, self.mean_vec)\n",
        "        cov = cov_matrix(centered)\n",
        "        eigs = eigen_decomp_2x2(cov)\n",
        "        eigs.sort(key=lambda x: -x[0])  # Sort by eigenvalue descending\n",
        "        self.components = [vec for val, vec in eigs[:self.n_components]]\n",
        "        return [[sum(row[j] * self.components[0][j] for j in range(len(row)))] for row in centered]\n",
        "\n",
        "\n",
        "# Run test cases\n",
        "# SVM test\n",
        "X_svm = [[1, 2], [2, 3], [3, 3], [2, 1], [3, 2]]\n",
        "y_svm = [0, 0, 0, 1, 1]\n",
        "model_svm = SimpleSVM()\n",
        "model_svm.fit(X_svm, y_svm)\n",
        "svm_result = model_svm.predict([[2, 2], [1, 1]])\n",
        "\n",
        "# PCA test\n",
        "X_pca = [[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0]]\n",
        "model_pca = SimplePCA(n_components=1)\n",
        "pca_result = model_pca.fit_transform(X_pca)\n",
        "pca_result_flat = [round(val[0], 2) for val in pca_result]  # flatten and round for readability\n",
        "\n",
        "(svm_result, pca_result_flat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4_t125Aai8T",
        "outputId": "1e0b0c08-7cbd-485e-8835-a62cc6d3d265"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0, 1], [0.51, -2.52, 0.66, -0.15, 1.5])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention:\n",
        "    def __init__(self, embed_dim):\n",
        "        self.embed_dim = embed_dim\n",
        "        self.W_q = [[0.1 for _ in range(embed_dim)] for _ in range(embed_dim)]\n",
        "        self.W_k = [[0.1 for _ in range(embed_dim)] for _ in range(embed_dim)]\n",
        "        self.W_v = [[0.1 for _ in range(embed_dim)] for _ in range(embed_dim)]\n",
        "\n",
        "    def dot(self, a, b):\n",
        "        return sum(x * y for x, y in zip(a, b))\n",
        "\n",
        "    def matvec(self, mat, vec):\n",
        "        return [self.dot(row, vec) for row in mat]\n",
        "\n",
        "    def softmax(self, x):\n",
        "        max_x = max(x)\n",
        "        exps = [pow(2.718, i - max_x) for i in x]\n",
        "        sum_exps = sum(exps)\n",
        "        return [j / sum_exps for j in exps]\n",
        "\n",
        "    def attention(self, q, k, v):\n",
        "        scores = [self.dot(q, ki) for ki in k]\n",
        "        weights = self.softmax(scores)\n",
        "        output = [0.0 for _ in v[0]]\n",
        "        for i in range(len(v)):\n",
        "            for j in range(len(v[0])):\n",
        "                output[j] += weights[i] * v[i][j]\n",
        "        return output\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        Q = [self.matvec(self.W_q, x) for x in inputs]\n",
        "        K = [self.matvec(self.W_k, x) for x in inputs]\n",
        "        V = [self.matvec(self.W_v, x) for x in inputs]\n",
        "        return [self.attention(q, K, V) for q in Q]\n",
        "\n",
        "\n",
        "# Multi-head Attention (2 heads for simplicity)\n",
        "class MultiHeadAttention:\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        self.heads = [SelfAttention(embed_dim) for _ in range(num_heads)]\n",
        "        self.num_heads = num_heads\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        head_outputs = [head.forward(inputs) for head in self.heads]\n",
        "        combined = []\n",
        "        for i in range(len(inputs)):\n",
        "            merged = []\n",
        "            for h in head_outputs:\n",
        "                merged += h[i]\n",
        "            combined.append(merged[:self.embed_dim])  # truncate for simplicity\n",
        "        return combined\n",
        "\n",
        "\n",
        "# Simplified Transformer Encoder block (Self-Attention + Add & Norm)\n",
        "class TransformerEncoder:\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        self.mha = MultiHeadAttention(embed_dim, num_heads)\n",
        "\n",
        "    def add_and_norm(self, x, sublayer_out):\n",
        "        return [[x[i][j] + sublayer_out[i][j] for j in range(len(x[0]))] for i in range(len(x))]\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        attention_out = self.mha.forward(inputs)\n",
        "        return self.add_and_norm(inputs, attention_out)\n",
        "\n",
        "\n",
        "# Input test data\n",
        "input_data = [[1.0, 0.0, 1.0], [0.0, 2.0, 0.0], [1.0, 1.0, 1.0]]\n",
        "sa = SelfAttention(embed_dim=3)\n",
        "self_attn_result = sa.forward(input_data)\n",
        "\n",
        "mha = MultiHeadAttention(embed_dim=3, num_heads=2)\n",
        "mha_result = mha.forward(input_data)\n",
        "\n",
        "encoder = TransformerEncoder(embed_dim=3, num_heads=2)\n",
        "encoder_result = encoder.forward(input_data)\n",
        "\n",
        "self_attn_result, mha_result, encoder_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4_tZQbUbCPV",
        "outputId": "00299549-7c26-443a-a6d0-42f0d7a56a64"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[0.23467958578183762, 0.23467958578183762, 0.23467958578183762],\n",
              "  [0.23467958578183762, 0.23467958578183762, 0.23467958578183762],\n",
              "  [0.2353621866011892, 0.2353621866011892, 0.2353621866011892]],\n",
              " [[0.23467958578183762, 0.23467958578183762, 0.23467958578183762],\n",
              "  [0.23467958578183762, 0.23467958578183762, 0.23467958578183762],\n",
              "  [0.2353621866011892, 0.2353621866011892, 0.2353621866011892]],\n",
              " [[1.2346795857818376, 0.23467958578183762, 1.2346795857818376],\n",
              "  [0.23467958578183762, 2.2346795857818376, 0.23467958578183762],\n",
              "  [1.2353621866011892, 1.2353621866011892, 1.2353621866011892]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention:\n",
        "    def __init__(self, embed_dim):\n",
        "        self.embed_dim = embed_dim\n",
        "        self.W_q = [[0.1]*embed_dim for _ in range(embed_dim)]\n",
        "        self.W_k = [[0.2]*embed_dim for _ in range(embed_dim)]\n",
        "        self.W_v = [[0.3]*embed_dim for _ in range(embed_dim)]\n",
        "\n",
        "    def dot(self, a, b):\n",
        "        return sum(x * y for x, y in zip(a, b))\n",
        "\n",
        "    def matvec(self, mat, vec):\n",
        "        return [self.dot(row, vec) for row in mat]\n",
        "\n",
        "    def softmax(self, x):\n",
        "        max_x = max(x)\n",
        "        exp = [pow(2.718, i - max_x) for i in x]\n",
        "        total = sum(exp)\n",
        "        return [i / total for i in exp]\n",
        "\n",
        "    def attention(self, q, K, V):\n",
        "        scores = [self.dot(q, k) for k in K]\n",
        "        weights = self.softmax(scores)\n",
        "        out = [0.0] * len(V[0])\n",
        "        for i in range(len(weights)):\n",
        "            for j in range(len(V[0])):\n",
        "                out[j] += weights[i] * V[i][j]\n",
        "        return out\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        Q = [self.matvec(self.W_q, x) for x in inputs]\n",
        "        K = [self.matvec(self.W_k, x) for x in inputs]\n",
        "        V = [self.matvec(self.W_v, x) for x in inputs]\n",
        "        return [self.attention(q, K, V) for q in Q]\n"
      ],
      "metadata": {
        "id": "ET-zZh-0bQWS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention:\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        self.heads = [SelfAttention(embed_dim) for _ in range(num_heads)]\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        head_outputs = [head.forward(inputs) for head in self.heads]\n",
        "        combined = []\n",
        "        for i in range(len(inputs)):\n",
        "            concat = []\n",
        "            for h in head_outputs:\n",
        "                concat += h[i]\n",
        "            combined.append(concat[:len(inputs[0])])  # truncate for simplicity\n",
        "        return combined\n"
      ],
      "metadata": {
        "id": "PLaQPMbcb5ql"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder:\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        self.mha = MultiHeadAttention(embed_dim, num_heads)\n",
        "\n",
        "    def add_and_norm(self, x, sublayer_out):\n",
        "        return [[x[i][j] + sublayer_out[i][j] for j in range(len(x[0]))] for i in range(len(x))]\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        attn_output = self.mha.forward(inputs)\n",
        "        return self.add_and_norm(inputs, attn_output)\n"
      ],
      "metadata": {
        "id": "AgULNk3Eb8KJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = [[1, 0, 1], [0, 2, 0], [1, 1, 1]]\n",
        "\n",
        "sa = SelfAttention(embed_dim=3)\n",
        "print(\"Self Attention:\", sa.forward(input_data))\n",
        "\n",
        "mha = MultiHeadAttention(embed_dim=3, num_heads=2)\n",
        "print(\"Multi-Head Attention:\", mha.forward(input_data))\n",
        "\n",
        "encoder = TransformerEncoder(embed_dim=3, num_heads=2)\n",
        "print(\"Transformer Encoder Output:\", encoder.forward(input_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXcPwwYIb9Ty",
        "outputId": "8e9e7e7d-1e73-4f26-f9ee-88bc80c7c1e9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self Attention: [[0.7081524235578396, 0.7081524235578396, 0.7081524235578396], [0.7081524235578396, 0.7081524235578396, 0.7081524235578396], [0.7123355014792846, 0.7123355014792846, 0.7123355014792846]]\n",
            "Multi-Head Attention: [[0.7081524235578396, 0.7081524235578396, 0.7081524235578396], [0.7081524235578396, 0.7081524235578396, 0.7081524235578396], [0.7123355014792846, 0.7123355014792846, 0.7123355014792846]]\n",
            "Transformer Encoder Output: [[1.7081524235578396, 0.7081524235578396, 1.7081524235578396], [0.7081524235578396, 2.7081524235578396, 0.7081524235578396], [1.7123355014792847, 1.7123355014792847, 1.7123355014792847]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN:\n",
        "    def __init__(self, kernel_size=3):\n",
        "        self.kernel_size = kernel_size\n",
        "        # Initialize a simple kernel with fixed weights\n",
        "        self.kernel = [0.2] * kernel_size\n",
        "\n",
        "    def conv1d(self, x):\n",
        "        n = len(x)\n",
        "        k = self.kernel_size\n",
        "        output = []\n",
        "        for i in range(n - k + 1):\n",
        "            s = 0\n",
        "            for j in range(k):\n",
        "                s += x[i + j] * self.kernel[j]\n",
        "            output.append(s)\n",
        "        return output\n",
        "\n",
        "    def relu(self, x):\n",
        "        return [max(0, i) for i in x]\n",
        "\n",
        "    def max_pool(self, x, pool_size=2, stride=2):\n",
        "        pooled = []\n",
        "        for i in range(0, len(x) - pool_size + 1, stride):\n",
        "            pooled.append(max(x[i:i + pool_size]))\n",
        "        return pooled\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv1d(x)\n",
        "        relu_out = self.relu(conv_out)\n",
        "        pooled = self.max_pool(relu_out)\n",
        "        return pooled\n",
        "cnn = SimpleCNN()\n",
        "input_signal = [1, 2, 3, 4, 5, 6]\n",
        "print(\"CNN output:\", cnn.forward(input_signal))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aMCRusJb--7",
        "outputId": "80976d22-96d1-4c3e-f2b4-81c580bb2dd3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN output: [1.8, 3.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNN:\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # Random-ish initialization of weights\n",
        "        self.W_xh = [[0.1 for _ in range(input_dim)] for _ in range(hidden_dim)]\n",
        "        self.W_hh = [[0.1 for _ in range(hidden_dim)] for _ in range(hidden_dim)]\n",
        "        self.b_h = [0.0 for _ in range(hidden_dim)]\n",
        "\n",
        "    def dot(self, a, b):\n",
        "        return sum(x * y for x, y in zip(a, b))\n",
        "\n",
        "    def matvec(self, mat, vec):\n",
        "        return [self.dot(row, vec) for row in mat]\n",
        "\n",
        "    def tanh(self, x):\n",
        "        # Approximate tanh with math.tanh or simple version\n",
        "        import math\n",
        "        return [math.tanh(i) for i in x]\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        h = [0.0] * self.hidden_dim\n",
        "        outputs = []\n",
        "        for x in inputs:\n",
        "            x_vec = [x] if not isinstance(x, list) else x\n",
        "            h_next = self.matvec(self.W_xh, x_vec)\n",
        "            h_rec = self.matvec(self.W_hh, h)\n",
        "            h = self.tanh([h_next[i] + h_rec[i] + self.b_h[i] for i in range(self.hidden_dim)])\n",
        "            outputs.append(h)\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "Ihg5N6TMcQI6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = SimpleRNN(input_dim=1, hidden_dim=2)\n",
        "seq = [[1], [2], [3]]\n",
        "print(\"RNN output:\", rnn.forward(seq))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdUUG841cTbS",
        "outputId": "854fc590-8872-411c-ad79-8417957ee969"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN output: [[0.09966799462495582, 0.09966799462495582], [0.21645477239531144, 0.21645477239531144], [0.33041224905058686, 0.33041224905058686]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical Functions from Scratch\n",
        "\n",
        "def mean(data):\n",
        "    return sum(data) / len(data)\n",
        "\n",
        "def variance(data):\n",
        "    m = mean(data)\n",
        "    return sum((x - m) ** 2 for x in data) / (len(data) - 1)  # sample variance\n",
        "\n",
        "def std_dev(data):\n",
        "    return variance(data) ** 0.5\n",
        "\n",
        "def covariance(x, y):\n",
        "    mean_x = mean(x)\n",
        "    mean_y = mean(y)\n",
        "    n = len(x)\n",
        "    return sum((x[i] - mean_x) * (y[i] - mean_y) for i in range(n)) / (n - 1)\n",
        "\n",
        "def correlation(x, y):\n",
        "    return covariance(x, y) / (std_dev(x) * std_dev(y))\n",
        "\n",
        "def skewness(data):\n",
        "    m = mean(data)\n",
        "    s = std_dev(data)\n",
        "    n = len(data)\n",
        "    return (sum((x - m) ** 3 for x in data) / n) / (s ** 3)\n",
        "\n",
        "def kurtosis(data):\n",
        "    m = mean(data)\n",
        "    s = std_dev(data)\n",
        "    n = len(data)\n",
        "    return (sum((x - m) ** 4 for x in data) / n) / (s ** 4) - 3\n",
        "\n",
        "def z_scores(data):\n",
        "    m = mean(data)\n",
        "    s = std_dev(data)\n",
        "    return [(x - m) / s for x in data]\n",
        "\n",
        "def covariance_matrix(matrix):\n",
        "    n = len(matrix)\n",
        "    dim = len(matrix[0])\n",
        "    means = [mean([matrix[i][j] for i in range(n)]) for j in range(dim)]\n",
        "    cov_mat = [[0] * dim for _ in range(dim)]\n",
        "    for i in range(dim):\n",
        "        for j in range(dim):\n",
        "            cov_mat[i][j] = sum((matrix[k][i] - means[i]) * (matrix[k][j] - means[j]) for k in range(n)) / (n - 1)\n",
        "    return cov_mat\n",
        "\n",
        "def confusion_matrix(y_true, y_pred, labels=None):\n",
        "    if labels is None:\n",
        "        labels = list(set(y_true + y_pred))\n",
        "    matrix = [[0]*len(labels) for _ in labels]\n",
        "    label_index = {label: idx for idx, label in enumerate(labels)}\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        matrix[label_index[t]][label_index[p]] += 1\n",
        "    return matrix\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    correct = sum(t == p for t, p in zip(y_true, y_pred))\n",
        "    return correct / len(y_true)\n",
        "\n",
        "def precision(y_true, y_pred, positive_label):\n",
        "    tp = sum((t == positive_label and p == positive_label) for t, p in zip(y_true, y_pred))\n",
        "    fp = sum((t != positive_label and p == positive_label) for t, p in zip(y_true, y_pred))\n",
        "    return tp / (tp + fp) if tp + fp > 0 else 0\n",
        "\n",
        "def recall(y_true, y_pred, positive_label):\n",
        "    tp = sum((t == positive_label and p == positive_label) for t, p in zip(y_true, y_pred))\n",
        "    fn = sum((t == positive_label and p != positive_label) for t, p in zip(y_true, y_pred))\n",
        "    return tp / (tp + fn) if tp + fn > 0 else 0\n",
        "\n",
        "def f1_score(y_true, y_pred, positive_label):\n",
        "    p = precision(y_true, y_pred, positive_label)\n",
        "    r = recall(y_true, y_pred, positive_label)\n",
        "    return 2 * p * r / (p + r) if p + r > 0 else 0\n",
        "\n",
        "def bias_variance_decomposition(predictions, true_values):\n",
        "    n = len(true_values)\n",
        "    mean_pred = mean(predictions)\n",
        "    bias_sq = (mean_pred - mean(true_values)) ** 2\n",
        "    variance_pred = variance(predictions)\n",
        "    noise = variance(true_values)  # Assuming noise = variance of true_values\n",
        "    return bias_sq, variance_pred, noise\n",
        "\n",
        "def cross_validation_split(data, k):\n",
        "    n = len(data)\n",
        "    fold_size = n // k\n",
        "    return [data[i*fold_size:(i+1)*fold_size] for i in range(k)]\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    n = len(y_true)\n",
        "    return sum((y_true[i] - y_pred[i]) ** 2 for i in range(n)) / n\n",
        "\n",
        "def bootstrap_sampling(data, n_samples):\n",
        "    import random\n",
        "    samples = []\n",
        "    for _ in range(n_samples):\n",
        "        sample = [random.choice(data) for _ in range(len(data))]\n",
        "        samples.append(sample)\n",
        "    return samples\n",
        "\n",
        "def hypothesis_test_t_stat(sample1, sample2):\n",
        "    m1, m2 = mean(sample1), mean(sample2)\n",
        "    v1, v2 = variance(sample1), variance(sample2)\n",
        "    n1, n2 = len(sample1), len(sample2)\n",
        "    se = ((v1/n1) + (v2/n2)) ** 0.5\n",
        "    return (m1 - m2) / se\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    data1 = [2, 4, 6, 8, 10]\n",
        "    data2 = [1, 3, 5, 7, 9]\n",
        "\n",
        "    print(\"Mean:\", mean(data1))\n",
        "    print(\"Variance:\", variance(data1))\n",
        "    print(\"Standard Deviation:\", std_dev(data1))\n",
        "    print(\"Covariance:\", covariance(data1, data2))\n",
        "    print(\"Correlation:\", correlation(data1, data2))\n",
        "    print(\"Skewness:\", skewness(data1))\n",
        "    print(\"Kurtosis:\", kurtosis(data1))\n",
        "    print(\"Z-Scores:\", z_scores(data1))\n",
        "\n",
        "    matrix = [[2,3], [4,5], [6,7]]\n",
        "    print(\"Covariance Matrix:\", covariance_matrix(matrix))\n",
        "\n",
        "    y_true = [1, 0, 1, 1, 0]\n",
        "    y_pred = [1, 0, 0, 1, 0]\n",
        "    print(\"Confusion Matrix:\", confusion_matrix(y_true, y_pred))\n",
        "    print(\"Accuracy:\", accuracy(y_true, y_pred))\n",
        "    print(\"Precision (1):\", precision(y_true, y_pred, 1))\n",
        "    print(\"Recall (1):\", recall(y_true, y_pred, 1))\n",
        "    print(\"F1 Score (1):\", f1_score(y_true, y_pred, 1))\n",
        "\n",
        "    preds = [3, 5, 7]\n",
        "    true_vals = [2, 5, 8]\n",
        "    print(\"Bias, Variance, Noise:\", bias_variance_decomposition(preds, true_vals))\n",
        "\n",
        "    print(\"Cross-validation folds:\", cross_validation_split(data1, 3))\n",
        "\n",
        "    print(\"Mean Squared Error:\", mean_squared_error(true_vals, preds))\n",
        "\n",
        "    samples = bootstrap_sampling(data1, 3)\n",
        "    print(\"Bootstrap samples:\", samples)\n",
        "\n",
        "    t_stat = hypothesis_test_t_stat(data1, data2)\n",
        "    print(\"T-test statistic:\", t_stat)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zERpfx4ocUdR",
        "outputId": "63bdd5ae-061f-41fc-fe6c-86f1395edcfc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: 6.0\n",
            "Variance: 10.0\n",
            "Standard Deviation: 3.1622776601683795\n",
            "Covariance: 10.0\n",
            "Correlation: 0.9999999999999998\n",
            "Skewness: 0.0\n",
            "Kurtosis: -1.9120000000000004\n",
            "Z-Scores: [-1.2649110640673518, -0.6324555320336759, 0.0, 0.6324555320336759, 1.2649110640673518]\n",
            "Covariance Matrix: [[4.0, 4.0], [4.0, 4.0]]\n",
            "Confusion Matrix: [[2, 0], [1, 2]]\n",
            "Accuracy: 0.8\n",
            "Precision (1): 1.0\n",
            "Recall (1): 0.6666666666666666\n",
            "F1 Score (1): 0.8\n",
            "Bias, Variance, Noise: (0.0, 4.0, 9.0)\n",
            "Cross-validation folds: [[2], [4], [6]]\n",
            "Mean Squared Error: 0.6666666666666666\n",
            "Bootstrap samples: [[10, 10, 4, 6, 8], [2, 10, 8, 2, 8], [4, 4, 2, 8, 4]]\n",
            "T-test statistic: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def dot_product(vec1, vec2):\n",
        "    return sum(x * y for x, y in zip(vec1, vec2))\n",
        "\n",
        "def matmul(A, B):\n",
        "    \"\"\"Multiply matrix A (list of lists) with matrix B (list of lists)\"\"\"\n",
        "    result = []\n",
        "    rows_A = len(A)\n",
        "    cols_A = len(A[0])\n",
        "    rows_B = len(B)\n",
        "    cols_B = len(B[0])\n",
        "    assert cols_A == rows_B, \"Incompatible dimensions for multiplication\"\n",
        "    for i in range(rows_A):\n",
        "        row_result = []\n",
        "        for j in range(cols_B):\n",
        "            s = 0\n",
        "            for k in range(cols_A):\n",
        "                s += A[i][k] * B[k][j]\n",
        "            row_result.append(s)\n",
        "        result.append(row_result)\n",
        "    return result\n",
        "\n",
        "def transpose(matrix):\n",
        "    return list(map(list, zip(*matrix)))\n",
        "\n",
        "def softmax(x):\n",
        "    max_x = max(x)\n",
        "    exps = [math.exp(i - max_x) for i in x]\n",
        "    sum_exps = sum(exps)\n",
        "    return [j / sum_exps for j in exps]\n",
        "\n",
        "def scale_dot_product_attention(Q, K, V):\n",
        "    \"\"\"\n",
        "    Q, K, V are lists of vectors:\n",
        "    Q: query vectors (n_q x d)\n",
        "    K: key vectors (n_k x d)\n",
        "    V: value vectors (n_k x d_v)\n",
        "    \"\"\"\n",
        "    d_k = len(K[0])\n",
        "    # Calculate scores = Q x K^T\n",
        "    K_T = transpose(K)\n",
        "    scores = matmul(Q, K_T)  # shape: (n_q x n_k)\n",
        "\n",
        "    # Scale scores\n",
        "    scaled_scores = []\n",
        "    scale_factor = math.sqrt(d_k)\n",
        "    for row in scores:\n",
        "        scaled_scores.append([x / scale_factor for x in row])\n",
        "\n",
        "    # Apply softmax to each row (query)\n",
        "    attention_weights = []\n",
        "    for row in scaled_scores:\n",
        "        attention_weights.append(softmax(row))\n",
        "\n",
        "    # Multiply weights by V\n",
        "    output = []\n",
        "    for weights in attention_weights:\n",
        "        out_vec = [0] * len(V[0])\n",
        "        for w, v in zip(weights, V):\n",
        "            for i in range(len(v)):\n",
        "                out_vec[i] += w * v[i]\n",
        "        output.append(out_vec)\n",
        "    return output, attention_weights\n",
        "\n",
        "class MultiHeadAttention:\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        assert embed_dim % num_heads == 0\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Initialize weights (Q,K,V) for each head, simple fixed weights for demo\n",
        "        # For simplicity, we use identity-like weights, i.e. no change on vectors\n",
        "        self.W_Q = [self.identity_matrix(self.head_dim) for _ in range(num_heads)]\n",
        "        self.W_K = [self.identity_matrix(self.head_dim) for _ in range(num_heads)]\n",
        "        self.W_V = [self.identity_matrix(self.head_dim) for _ in range(num_heads)]\n",
        "        # Output linear projection weights (identity for simplicity)\n",
        "        self.W_O = self.identity_matrix(embed_dim)\n",
        "\n",
        "    def identity_matrix(self, dim):\n",
        "        return [[1 if i == j else 0 for j in range(dim)] for i in range(dim)]\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # x shape: (seq_len x embed_dim)\n",
        "        split = []\n",
        "        for i in range(self.num_heads):\n",
        "            head = []\n",
        "            for vec in x:\n",
        "                head.append(vec[i * self.head_dim:(i + 1) * self.head_dim])\n",
        "            split.append(head)\n",
        "        return split\n",
        "\n",
        "    def combine_heads(self, heads):\n",
        "        # heads: list of num_heads elements each (seq_len x head_dim)\n",
        "        combined = []\n",
        "        seq_len = len(heads[0])\n",
        "        for i in range(seq_len):\n",
        "            combined_vec = []\n",
        "            for head in heads:\n",
        "                combined_vec.extend(head[i])\n",
        "            combined.append(combined_vec)\n",
        "        return combined\n",
        "\n",
        "    def linear(self, x, W):\n",
        "        # x: list of vectors; W: square matrix (dim x dim)\n",
        "        return [ [dot_product(W_row, vec) for W_row in transpose(W)] for vec in x]\n",
        "\n",
        "    def forward(self, Q, K, V):\n",
        "        # Split into heads\n",
        "        Q_heads = self.split_heads(Q)\n",
        "        K_heads = self.split_heads(K)\n",
        "        V_heads = self.split_heads(V)\n",
        "\n",
        "        attention_outputs = []\n",
        "        for i in range(self.num_heads):\n",
        "            # Linear projections (identity here for demo)\n",
        "            q_proj = self.linear(Q_heads[i], self.W_Q[i])\n",
        "            k_proj = self.linear(K_heads[i], self.W_K[i])\n",
        "            v_proj = self.linear(V_heads[i], self.W_V[i])\n",
        "\n",
        "            attn_out, _ = scale_dot_product_attention(q_proj, k_proj, v_proj)\n",
        "            attention_outputs.append(attn_out)\n",
        "\n",
        "        # Combine heads\n",
        "        concat = self.combine_heads(attention_outputs)\n",
        "        # Final linear projection (identity)\n",
        "        output = self.linear(concat, self.W_O)\n",
        "        return output\n",
        "\n",
        "class TransformerEncoderBlock:\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        self.mha = MultiHeadAttention(embed_dim, num_heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Self-attention\n",
        "        attn_output = self.mha.forward(x, x, x)  # Q=K=V=x\n",
        "        # Add & Norm skipped for simplicity\n",
        "        return attn_output\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Input: sequence length=2, embedding dimension=4\n",
        "    x = [\n",
        "        [1, 0, 1, 0],\n",
        "        [0, 2, 0, 2]\n",
        "    ]\n",
        "    encoder = TransformerEncoderBlock(embed_dim=4, num_heads=2)\n",
        "    output = encoder.forward(x)\n",
        "    print(\"Transformer Encoder output:\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-Ml8aTncmnO",
        "outputId": "f2d8bbed-a858-4041-ee86-faca53777d38"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer Encoder output: [[0.6697615493266569, 0.6604769013466862, 0.6697615493266569, 0.6604769013466862], [0.055807219207169745, 1.8883855615856606, 0.055807219207169745, 1.8883855615856606]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from math import log, exp\n",
        "\n",
        "##########################\n",
        "# 1. Naive Bayes (Categorical) from scratch\n",
        "##########################\n",
        "class NaiveBayes:\n",
        "    def __init__(self):\n",
        "        self.class_probs = {}\n",
        "        self.feature_probs = {}\n",
        "        self.classes = set()\n",
        "        self.features = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes = set(y)\n",
        "        n = len(y)\n",
        "        self.features = list(range(len(X[0])))\n",
        "\n",
        "        # Prior probabilities P(c)\n",
        "        self.class_probs = {c: sum(1 for label in y if label == c) / n for c in self.classes}\n",
        "\n",
        "        # Conditional probabilities P(x_i | c)\n",
        "        self.feature_probs = {c: defaultdict(lambda: defaultdict(int)) for c in self.classes}\n",
        "        class_counts = defaultdict(int)\n",
        "\n",
        "        for xi, label in zip(X, y):\n",
        "            class_counts[label] += 1\n",
        "            for i, val in enumerate(xi):\n",
        "                self.feature_probs[label][i][val] += 1\n",
        "\n",
        "        # Convert counts to probabilities with Laplace smoothing\n",
        "        for c in self.classes:\n",
        "            for i in self.features:\n",
        "                total = class_counts[c] + len(self.feature_probs[c][i])\n",
        "                for val in self.feature_probs[c][i]:\n",
        "                    self.feature_probs[c][i][val] = (self.feature_probs[c][i][val] + 1) / total\n",
        "\n",
        "    def predict(self, X):\n",
        "        preds = []\n",
        "        for xi in X:\n",
        "            class_scores = {}\n",
        "            for c in self.classes:\n",
        "                score = math.log(self.class_probs[c])\n",
        "                for i, val in enumerate(xi):\n",
        "                    prob = self.feature_probs[c][i].get(val, 1e-6)\n",
        "                    score += math.log(prob)\n",
        "                class_scores[c] = score\n",
        "            preds.append(max(class_scores, key=class_scores.get))\n",
        "        return preds\n",
        "\n",
        "\n",
        "##########################\n",
        "# 2. K-Nearest Neighbors (KNN)\n",
        "##########################\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "        self.X = []\n",
        "        self.y = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def _distance(self, a, b):\n",
        "        return math.sqrt(sum((x - y) ** 2 for x, y in zip(a, b)))\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        preds = []\n",
        "        for x in X_test:\n",
        "            distances = [(self._distance(x, xi), yi) for xi, yi in zip(self.X, self.y)]\n",
        "            distances.sort(key=lambda t: t[0])\n",
        "            nearest = distances[:self.k]\n",
        "            votes = defaultdict(int)\n",
        "            for _, label in nearest:\n",
        "                votes[label] += 1\n",
        "            preds.append(max(votes, key=votes.get))\n",
        "        return preds\n",
        "\n",
        "\n",
        "##########################\n",
        "# 3. K-Means Clustering\n",
        "##########################\n",
        "class KMeans:\n",
        "    def __init__(self, k=2, max_iters=100):\n",
        "        self.k = k\n",
        "        self.max_iters = max_iters\n",
        "        self.centroids = []\n",
        "\n",
        "    def _distance(self, a, b):\n",
        "        return math.sqrt(sum((x - y) ** 2 for x, y in zip(a, b)))\n",
        "\n",
        "    def fit(self, X):\n",
        "        self.centroids = random.sample(X, self.k)\n",
        "        for _ in range(self.max_iters):\n",
        "            clusters = [[] for _ in range(self.k)]\n",
        "            for x in X:\n",
        "                distances = [self._distance(x, c) for c in self.centroids]\n",
        "                idx = distances.index(min(distances))\n",
        "                clusters[idx].append(x)\n",
        "\n",
        "            new_centroids = []\n",
        "            for cluster in clusters:\n",
        "                if cluster:\n",
        "                    mean_centroid = [sum(dim) / len(cluster) for dim in zip(*cluster)]\n",
        "                else:\n",
        "                    mean_centroid = random.choice(X)\n",
        "                new_centroids.append(mean_centroid)\n",
        "\n",
        "            if new_centroids == self.centroids:\n",
        "                break\n",
        "            self.centroids = new_centroids\n",
        "\n",
        "    def predict(self, X):\n",
        "        preds = []\n",
        "        for x in X:\n",
        "            distances = [self._distance(x, c) for c in self.centroids]\n",
        "            preds.append(distances.index(min(distances)))\n",
        "        return preds\n",
        "\n",
        "\n",
        "##########################\n",
        "# 4. Torch-like MLP Basic (Pure Python, no torch)\n",
        "##########################\n",
        "class SimpleMLP:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        # Weights initialization\n",
        "        self.W1 = [[random.uniform(-0.1, 0.1) for _ in range(hidden_dim)] for _ in range(input_dim)]\n",
        "        self.b1 = [0.0] * hidden_dim\n",
        "        self.W2 = [[random.uniform(-0.1, 0.1) for _ in range(output_dim)] for _ in range(hidden_dim)]\n",
        "        self.b2 = [0.0] * output_dim\n",
        "\n",
        "    def relu(self, x):\n",
        "        return [max(0, i) for i in x]\n",
        "\n",
        "    def matmul(self, x, W):\n",
        "        return [sum(x[j] * W[j][i] for j in range(len(x))) for i in range(len(W[0]))]\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.relu([a + b for a, b in zip(self.matmul(x, self.W1), self.b1)])\n",
        "        out = [a + b for a, b in zip(self.matmul(h, self.W2), self.b2)]\n",
        "        return out\n",
        "\n",
        "\n",
        "##########################\n",
        "# 5. Attention with KV Cache (simplified)\n",
        "##########################\n",
        "class AttentionKVCache:\n",
        "    def __init__(self):\n",
        "        self.keys_cache = []\n",
        "        self.values_cache = []\n",
        "\n",
        "    def attend(self, q, k, v):\n",
        "        # q, k, v are lists of floats\n",
        "        scores = [dot_product(q, ki) for ki in k]\n",
        "        max_score = max(scores)\n",
        "        exps = [math.exp(s - max_score) for s in scores]\n",
        "        sum_exps = sum(exps)\n",
        "        weights = [e / sum_exps for e in exps]\n",
        "        out = [0] * len(v[0])\n",
        "        for w, vi in zip(weights, v):\n",
        "            for i in range(len(vi)):\n",
        "                out[i] += w * vi[i]\n",
        "        return out\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        # Append new keys and values to cache\n",
        "        self.keys_cache.extend(k)\n",
        "        self.values_cache.extend(v)\n",
        "        return self.attend(q, self.keys_cache, self.values_cache)\n",
        "\n",
        "\n",
        "##########################\n",
        "# 6. Logistic Regression (Binary)\n",
        "##########################\n",
        "class LogisticRegression:\n",
        "    def __init__(self, lr=0.1, epochs=100):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = 0\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + math.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n, d = len(X), len(X[0])\n",
        "        self.weights = [0.0] * d\n",
        "        for _ in range(self.epochs):\n",
        "            for xi, yi in zip(X, y):\n",
        "                z = sum(w * x for w, x in zip(self.weights, xi)) + self.bias\n",
        "                pred = self.sigmoid(z)\n",
        "                error = yi - pred\n",
        "                for j in range(d):\n",
        "                    self.weights[j] += self.lr * error * xi[j]\n",
        "                self.bias += self.lr * error\n",
        "\n",
        "    def predict(self, X):\n",
        "        preds = []\n",
        "        for xi in X:\n",
        "            z = sum(w * x for w, x in zip(self.weights, xi)) + self.bias\n",
        "            p = self.sigmoid(z)\n",
        "            preds.append(1 if p > 0.5 else 0)\n",
        "        return preds\n",
        "\n",
        "\n",
        "##########################\n",
        "# 7. TF-IDF from scratch\n",
        "##########################\n",
        "class TFIDF:\n",
        "    def __init__(self):\n",
        "        self.df = defaultdict(int)\n",
        "        self.idf = {}\n",
        "        self.vocab = set()\n",
        "        self.N = 0\n",
        "\n",
        "    def fit(self, docs):\n",
        "        self.N = len(docs)\n",
        "        for doc in docs:\n",
        "            words = set(doc)\n",
        "            for w in words:\n",
        "                self.df[w] += 1\n",
        "        for w, freq in self.df.items():\n",
        "            self.idf[w] = math.log((self.N + 1) / (freq + 1)) + 1\n",
        "\n",
        "    def transform(self, docs):\n",
        "        tfidf_docs = []\n",
        "        for doc in docs:\n",
        "            tf = defaultdict(int)\n",
        "            for w in doc:\n",
        "                tf[w] += 1\n",
        "            doc_len = len(doc)\n",
        "            tfidf = {}\n",
        "            for w in tf:\n",
        "                tfidf[w] = (tf[w] / doc_len) * self.idf.get(w, 0)\n",
        "            tfidf_docs.append(tfidf)\n",
        "        return tfidf_docs\n",
        "\n",
        "\n",
        "##########################\n",
        "# 8. Metrics (Accuracy, Precision, Recall, F1) -- simple binary example\n",
        "##########################\n",
        "def accuracy_score(y_true, y_pred):\n",
        "    return sum(t == p for t, p in zip(y_true, y_pred)) / len(y_true)\n",
        "\n",
        "def precision_score(y_true, y_pred):\n",
        "    tp = sum((t == 1 and p == 1) for t, p in zip(y_true, y_pred))\n",
        "    fp = sum((t == 0 and p == 1) for t, p in zip(y_true, y_pred))\n",
        "    return tp / (tp + fp) if tp + fp > 0 else 0\n",
        "\n",
        "def recall_score(y_true, y_pred):\n",
        "    tp = sum((t == 1 and p == 1) for t, p in zip(y_true, y_pred))\n",
        "    fn = sum((t == 1 and p == 0) for t, p in zip(y_true, y_pred))\n",
        "    return tp / (tp + fn) if tp + fn > 0 else 0\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    p = precision_score(y_true, y_pred)\n",
        "    r = recall_score(y_true, y_pred)\n",
        "    return 2 * p * r / (p + r) if p + r > 0 else 0"
      ],
      "metadata": {
        "id": "74_h_KQyc3Ig"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Zu_dBroc9u2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}